"""
Import EPFL course data from local CSV files into Supabase Postgres.

Usage:
  python supabase/import_from_csv.py \
    --supabase-url https://YOUR-PROJECT.supabase.co \
    --service-role-key <service-role-key>

Requirements:
  - Python 3.11+
  - pip install requests
  - CSV inputs generated by data-scraper:
      data-scraper/data/epfl_courses.csv
      data-scraper/data/courses_scores.csv

The script performs bulk upserts in this order:
  1. courses
  2. tags
  3. course_offerings
  4. course_tags

It matches the schema created by supabase/init_postgres.sql.
"""

from __future__ import annotations

import argparse
import ast
import csv
import json
import os
import sys
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, Iterable, List, Tuple

import requests


ROOT = Path(__file__).resolve().parents[1]
ENV_PATH = Path(__file__).resolve().parent / ".env"
DATA_DIR = ROOT / "data-scraper" / "data"
COURSES_CSV = DATA_DIR / "epfl_courses.csv"
SCORES_CSV = DATA_DIR / "courses_scores.csv"


def load_env_file(path: Path) -> None:
    if not path.exists():
        return
    for line in path.read_text(encoding="utf-8").splitlines():
        if not line or line.strip().startswith("#"):
            continue
        if "=" not in line:
            continue
        key, value = line.split("=", 1)
        key = key.strip()
        value = value.strip().strip('"').strip("'")
        if key and value and key not in os.environ:
            os.environ[key] = value


def parse_args() -> argparse.Namespace:
    load_env_file(ENV_PATH)
    parser = argparse.ArgumentParser(description="Import EPFL courses into Supabase")
    parser.add_argument("--supabase-url", default=os.getenv("SUPABASE_URL"), help="Supabase project URL")
    parser.add_argument(
        "--service-role-key",
        default=os.getenv("SUPABASE_SERVICE_ROLE_KEY"),
        help="Supabase service role key (required for inserts)",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=200,
        help="Number of rows per upsert batch (default: 200)",
    )
    return parser.parse_args()


class SupabaseClient:
    def __init__(self, base_url: str, service_role_key: str, batch_size: int = 200):
        self.base_url = base_url.rstrip("/")
        self.service_role_key = service_role_key
        self.batch_size = max(1, batch_size)
        self.session = requests.Session()
        self.default_headers = {
            "apikey": service_role_key,
            "Authorization": f"Bearer {service_role_key}",
            "Content-Type": "application/json",
            "Accept": "application/json",
        }

    def _request(
        self,
        method: str,
        path: str,
        *,
        params: Dict[str, Any] | None = None,
        headers: Dict[str, str] | None = None,
        data: Any | None = None,
    ) -> requests.Response:
        url = f"{self.base_url}{path}"
        all_headers = dict(self.default_headers)
        if headers:
            all_headers.update(headers)
        resp = self.session.request(
            method,
            url,
            params=params,
            data=data,
            headers=all_headers,
            timeout=60,
        )
        if not resp.ok:
            try:
                err = resp.json()
            except Exception:
                err = resp.text
            raise RuntimeError(f"{method} {path} failed: {resp.status_code} {err}")
        return resp

    def upsert(self, table: str, rows: Iterable[Dict[str, Any]], on_conflict: str) -> List[Dict[str, Any]]:
        all_rows = list(rows)
        results: List[Dict[str, Any]] = []
        if not all_rows:
            return results
        headers = {"Prefer": "resolution=merge-duplicates,return=representation"}
        params = {"on_conflict": on_conflict}
        for chunk in chunked(all_rows, self.batch_size):
            resp = self._request("POST", f"/rest/v1/{table}", params=params, headers=headers, data=json.dumps(chunk))
            results.extend(resp.json())
        return results

    def insert_ignore(self, table: str, rows: Iterable[Dict[str, Any]]) -> None:
        rows = list(rows)
        if not rows:
            return
        headers = {"Prefer": "resolution=ignore-duplicates"}
        for chunk in chunked(rows, self.batch_size):
            self._request("POST", f"/rest/v1/{table}", headers=headers, data=json.dumps(chunk))

    def select(self, table: str, *, filters: Dict[str, str] | None = None, columns: str = "*") -> List[Dict[str, Any]]:
        params = {"select": columns}
        if filters:
            params.update(filters)
        resp = self._request("GET", f"/rest/v1/{table}", params=params)
        return resp.json()

    def select_all(
        self,
        table: str,
        *,
        filters: Dict[str, str] | None = None,
        columns: str = "*",
        page_size: int = 1000,
    ) -> List[Dict[str, Any]]:
        """Fetch all rows by paging with Range headers (PostgREST default limit is 1000)."""
        rows: List[Dict[str, Any]] = []
        start = 0
        while True:
            end = start + page_size - 1
            params = {"select": columns}
            if filters:
                params.update(filters)
            resp = self._request(
                "GET",
                f"/rest/v1/{table}",
                params=params,
                headers={"Range-Unit": "items", "Range": f"{start}-{end}"},
            )
            batch = resp.json()
            rows.extend(batch)
            if len(batch) < page_size:
                break
            start += page_size
        return rows


def chunked(items: Iterable[Any], size: int) -> Iterable[List[Any]]:
    chunk: List[Any] = []
    for item in items:
        chunk.append(item)
        if len(chunk) >= size:
            yield chunk
            chunk = []
    if chunk:
        yield chunk


def parse_list_field(value: str) -> List[str]:
    if not value:
        return []
    try:
        parsed = ast.literal_eval(value)
    except (SyntaxError, ValueError):
        return []
    if isinstance(parsed, (list, tuple)):
        return [str(x).strip() for x in parsed if str(x).strip()]
    return []


def coalesce(*values: Any, fallback: Any) -> Any:
    for v in values:
        if isinstance(v, str):
            if v.strip():
                return v.strip()
        elif v is not None:
            return v
    return fallback


def load_scores(path: Path) -> Dict[str, Dict[str, Any]]:
    scores: Dict[str, Dict[str, Any]] = {}
    with path.open(newline="", encoding="utf-8") as handle:
        reader = csv.DictReader(handle)
        for row in reader:
            row_id = row.get("row_id")
            if not row_id:
                continue
            scores[row_id] = {
                "score_skills_sigmoid": parse_float(row.get("score_skills_sigmoid")),
                "score_product_sigmoid": parse_float(row.get("score_product_sigmoid")),
                "score_venture_sigmoid": parse_float(row.get("score_venture_sigmoid")),
                "score_foundations_sigmoid": parse_float(row.get("score_foundations_sigmoid")),
            }
    return scores


def parse_float(value: str | None) -> float | None:
    if value is None or value == "":
        return None
    try:
        return float(value)
    except ValueError:
        return None


def build_payloads(
    courses_csv: Path, scores_csv: Path
) -> Tuple[
    Dict[str, Dict[str, Any]],  # courses_map by course_code
    List[Dict[str, Any]],       # offerings rows (with course_code, row_id, ...)
    Dict[str, set[str]],        # keywords per course_code (for tag creation)
    Dict[str, set[str]],        # programs per course_code (for tag creation)
    set[Tuple[str, str, str]]   # offering_tag_links: (row_id, tag_type, tag_name)
]:
    if not courses_csv.exists():
        raise FileNotFoundError(f"Missing CSV {courses_csv}")
    if not scores_csv.exists():
        raise FileNotFoundError(f"Missing CSV {scores_csv}")

    scores_map = load_scores(scores_csv)

    courses_map: Dict[str, Dict[str, Any]] = {}
    offerings: List[Dict[str, Any]] = []
    keywords_map: Dict[str, set[str]] = defaultdict(set)
    programs_map: Dict[str, set[str]] = defaultdict(set)
    offering_tag_links: set[Tuple[str, str, str]] = set()

    with courses_csv.open(newline="", encoding="utf-8") as handle:
        reader = csv.DictReader(handle)
        for row in reader:
            course_code = row.get("course_code", "").strip()
            if not course_code:
                continue
            course = courses_map.setdefault(course_code, {
                "course_code": course_code,
                "course_name": row.get("course_name", "").strip() or course_code,
                "course_url": row.get("course_url", "").strip() or None,
                "credits": int(parse_float(row.get("credits")) or 0),
                "lang": coalesce(row.get("lang"), fallback="unknown"),
                "semester": coalesce(row.get("semester"), fallback="unknown"),
                "exam_form": row.get("exam_form", "").strip() or None,
                "workload": row.get("workload", "").strip() or None,
            })

            # Fill missing non-nullable fields if we find better values later
            if course.get("course_name") == course_code and row.get("course_name"):
                course["course_name"] = row["course_name"].strip()
            if course.get("course_url") is None and row.get("course_url"):
                course["course_url"] = row.get("course_url").strip() or None
            if course.get("lang") == "unknown" and row.get("lang"):
                course["lang"] = row["lang"].strip()
            if course.get("semester") == "unknown" and row.get("semester"):
                course["semester"] = row["semester"].strip()
            if not course.get("exam_form") and row.get("exam_form"):
                course["exam_form"] = row["exam_form"].strip()
            if not course.get("workload") and row.get("workload"):
                course["workload"] = row["workload"].strip()

            row_id = row.get("row_id", "").strip()
            if row_id:
                score = scores_map.get(row_id, {})
                offerings.append({
                    "course_code": course_code,
                    "row_id": row_id,
                    "section": row.get("section", "").strip() or "",
                    "type": row.get("type", "mandatory").strip() or "mandatory",
                    "prof_name": row.get("prof_name", "").strip() or None,
                    **{k: score.get(k) for k in [
                        "score_skills_sigmoid",
                        "score_product_sigmoid",
                        "score_venture_sigmoid",
                        "score_foundations_sigmoid",
                    ]},
                })

            keywords = parse_list_field(row.get("keywords", ""))
            for kw in keywords:
                keywords_map[course_code].add(kw)
                if row_id:
                    offering_tag_links.add((row_id, "keywords", kw))

            programs = parse_list_field(row.get("available_programs", ""))
            for prog in programs:
                programs_map[course_code].add(prog)
                if row_id:
                    offering_tag_links.add((row_id, "available_programs", prog))

    return courses_map, offerings, keywords_map, programs_map, offering_tag_links


def main() -> None:
    args = parse_args()
    if not args.supabase_url or not args.service_role_key:
        print("Supabase URL and service role key are required", file=sys.stderr)
        sys.exit(1)

    client = SupabaseClient(args.supabase_url, args.service_role_key, args.batch_size)

    courses_map, offerings, keywords_map, programs_map, offering_tag_links = build_payloads(COURSES_CSV, SCORES_CSV)

    print(f"Preparing to upsert {len(courses_map)} courses, {len(offerings)} offerings")

    # 1. Upsert courses and capture IDs
    course_rows = list(courses_map.values())
    upserted_courses = client.upsert("courses", course_rows, on_conflict="course_code")
    course_ids: Dict[str, int] = {}
    for row in upserted_courses:
        course_ids[row["course_code"]] = row["id"]
    if len(course_ids) != len(courses_map):
        print("Warning: upserted course count mismatch; fetching remaining ids")
        rows = client.select("courses", columns="id,course_code")
        for row in rows:
            if row["course_code"] in courses_map:
                course_ids[row["course_code"]] = row["id"]

    # 2. Ensure tags exist
    tag_types = {item["name"]: item["id"] for item in client.select("tag_types", columns="id,name")}
    missing_types = {"keywords", "available_programs"} - tag_types.keys()
    if missing_types:
        raise RuntimeError(f"Missing tag types in Supabase: {missing_types}")

    tag_rows = []
    tag_pairs: set[Tuple[int, str]] = set()
    for _course_code, tags in keywords_map.items():
        for name in tags:
            key = (tag_types["keywords"], name)
            if key in tag_pairs:
                continue
            tag_pairs.add(key)
            tag_rows.append({"tag_type_id": key[0], "name": name})
    for _course_code, tags in programs_map.items():
        for name in tags:
            key = (tag_types["available_programs"], name)
            if key in tag_pairs:
                continue
            tag_pairs.add(key)
            tag_rows.append({"tag_type_id": key[0], "name": name})

    if tag_rows:
        client.upsert("tags", tag_rows, on_conflict="tag_type_id,name")

    # Fetch tag IDs for mapping (page through all rows to avoid 1000-row default cap)
    all_tags = client.select_all("tags", columns="id,name,tag_type_id")
    tag_lookup: Dict[Tuple[int, str], int] = {}
    for tag in all_tags:
        tag_lookup[(tag["tag_type_id"], tag["name"])] = tag["id"]

    # 3. Upsert course_offerings with resolved course_id
    offering_rows = []
    for item in offerings:
        course_id = course_ids.get(item["course_code"])
        if not course_id:
            continue
        offering_rows.append({
            "course_id": course_id,
            "row_id": item["row_id"],
            "section": item["section"],
            "type": item["type"],
            "prof_name": item["prof_name"],
            "score_skills_sigmoid": item["score_skills_sigmoid"],
            "score_product_sigmoid": item["score_product_sigmoid"],
            "score_venture_sigmoid": item["score_venture_sigmoid"],
            "score_foundations_sigmoid": item["score_foundations_sigmoid"],
        })

    if offering_rows:
        client.upsert("course_offerings", offering_rows, on_conflict="row_id")

    # 4. Link offerings to tags (offering_tags)
    #    First, resolve offering row_id -> offering id
    offering_rows_all = client.select_all("course_offerings", columns="id,row_id")
    rowid_to_offid: Dict[str, int] = {r["row_id"]: r["id"] for r in offering_rows_all}

    offering_tag_rows = []
    seen_pairs: set[Tuple[int, int]] = set()
    for row_id, tag_type, tag_name in offering_tag_links:
        off_id = rowid_to_offid.get(row_id)
        tag_type_id = tag_types.get(tag_type)
        tag_id = tag_lookup.get((tag_type_id, tag_name))
        if not off_id or not tag_id:
            continue
        key = (off_id, tag_id)
        if key in seen_pairs:
            continue
        seen_pairs.add(key)
        offering_tag_rows.append({"offering_id": off_id, "tag_id": tag_id})

    if offering_tag_rows:
        client.insert_ignore("offering_tags", offering_tag_rows)

    # Summary
    prog_count = sum(1 for _r, _t, _n in offering_tag_links if _t == "available_programs")
    kw_count = sum(1 for _r, _t, _n in offering_tag_links if _t == "keywords")
    print(
        f"Linked offering↔tags: programs={prog_count} (rows inserted may be fewer due to dedupe), keywords={kw_count}"
    )

    print("Import completed")


if __name__ == "__main__":
    try:
        main()
    except Exception as exc:  # pylint: disable=broad-except
        print(f"Error: {exc}", file=sys.stderr)
        sys.exit(1)
