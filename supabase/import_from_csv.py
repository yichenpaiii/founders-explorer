"""
Import EPFL course data from local CSV files into Supabase Postgres.

Usage:
  python supabase/import_from_csv.py \
    --supabase-url https://YOUR-PROJECT.supabase.co \
    --service-role-key <service-role-key>

Requirements:
  - Python 3.11+
  - pip install requests
  - CSV inputs generated by data-scraper:
      data-scraper/data/epfl_courses.csv
      data-scraper/data/courses_scores.csv

The script performs bulk upserts in this order:
  1. courses
  2. tags (keywords)
  3. course_offerings
  4. offering_tags (keywords)
  5. programs
  6. levels
  7. offering_program_levels

It matches the schema created by supabase/init_postgres.sql.
"""

from __future__ import annotations

import argparse
import ast
import csv
import json
import os
import re
import sys
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, Iterable, List, Tuple

import requests


ROOT = Path(__file__).resolve().parents[1]
ENV_PATH = Path(__file__).resolve().parent / ".env"
DATA_DIR = ROOT / "data-scraper" / "data"
COURSES_CSV = DATA_DIR / "epfl_courses.csv"
SCORES_CSV = DATA_DIR / "courses_scores.csv"


def load_env_file(path: Path) -> None:
    if not path.exists():
        return
    for line in path.read_text(encoding="utf-8").splitlines():
        if not line or line.strip().startswith("#"):
            continue
        if "=" not in line:
            continue
        key, value = line.split("=", 1)
        key = key.strip()
        value = value.strip().strip('"').strip("'")
        if key and value and key not in os.environ:
            os.environ[key] = value


def parse_args() -> argparse.Namespace:
    load_env_file(ENV_PATH)
    parser = argparse.ArgumentParser(description="Import EPFL courses into Supabase")
    parser.add_argument("--supabase-url", default=os.getenv("SUPABASE_URL"), help="Supabase project URL")
    parser.add_argument(
        "--service-role-key",
        default=os.getenv("SUPABASE_SERVICE_ROLE_KEY"),
        help="Supabase service role key (required for inserts)",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=200,
        help="Number of rows per upsert batch (default: 200)",
    )
    return parser.parse_args()


class SupabaseClient:
    def __init__(self, base_url: str, service_role_key: str, batch_size: int = 200):
        self.base_url = base_url.rstrip("/")
        self.service_role_key = service_role_key
        self.batch_size = max(1, batch_size)
        self.session = requests.Session()
        self.default_headers = {
            "apikey": service_role_key,
            "Authorization": f"Bearer {service_role_key}",
            "Content-Type": "application/json",
            "Accept": "application/json",
        }

    def _request(
        self,
        method: str,
        path: str,
        *,
        params: Dict[str, Any] | None = None,
        headers: Dict[str, str] | None = None,
        data: Any | None = None,
    ) -> requests.Response:
        url = f"{self.base_url}{path}"
        all_headers = dict(self.default_headers)
        if headers:
            all_headers.update(headers)
        resp = self.session.request(
            method,
            url,
            params=params,
            data=data,
            headers=all_headers,
            timeout=60,
        )
        if not resp.ok:
            try:
                err = resp.json()
            except Exception:
                err = resp.text
            raise RuntimeError(f"{method} {path} failed: {resp.status_code} {err}")
        return resp

    def upsert(self, table: str, rows: Iterable[Dict[str, Any]], on_conflict: str) -> List[Dict[str, Any]]:
        all_rows = list(rows)
        results: List[Dict[str, Any]] = []
        if not all_rows:
            return results
        headers = {"Prefer": "resolution=merge-duplicates,return=representation"}
        params = {"on_conflict": on_conflict}
        for chunk in chunked(all_rows, self.batch_size):
            resp = self._request("POST", f"/rest/v1/{table}", params=params, headers=headers, data=json.dumps(chunk))
            results.extend(resp.json())
        return results

    def insert_ignore(self, table: str, rows: Iterable[Dict[str, Any]]) -> None:
        rows = list(rows)
        if not rows:
            return
        headers = {"Prefer": "resolution=ignore-duplicates"}
        for chunk in chunked(rows, self.batch_size):
            self._request("POST", f"/rest/v1/{table}", headers=headers, data=json.dumps(chunk))

    def select(self, table: str, *, filters: Dict[str, str] | None = None, columns: str = "*") -> List[Dict[str, Any]]:
        params = {"select": columns}
        if filters:
            params.update(filters)
        resp = self._request("GET", f"/rest/v1/{table}", params=params)
        return resp.json()

    def select_all(
        self,
        table: str,
        *,
        filters: Dict[str, str] | None = None,
        columns: str = "*",
        page_size: int = 1000,
    ) -> List[Dict[str, Any]]:
        """Fetch all rows by paging with Range headers (PostgREST default limit is 1000)."""
        rows: List[Dict[str, Any]] = []
        start = 0
        while True:
            end = start + page_size - 1
            params = {"select": columns}
            if filters:
                params.update(filters)
            resp = self._request(
                "GET",
                f"/rest/v1/{table}",
                params=params,
                headers={"Range-Unit": "items", "Range": f"{start}-{end}"},
            )
            batch = resp.json()
            rows.extend(batch)
            if len(batch) < page_size:
                break
            start += page_size
        return rows


def chunked(items: Iterable[Any], size: int) -> Iterable[List[Any]]:
    chunk: List[Any] = []
    for item in items:
        chunk.append(item)
        if len(chunk) >= size:
            yield chunk
            chunk = []
    if chunk:
        yield chunk


def parse_list_field(value: str) -> List[str]:
    if not value:
        return []
    try:
        parsed = ast.literal_eval(value)
    except (SyntaxError, ValueError):
        return []
    if isinstance(parsed, (list, tuple)):
        return [str(x).strip() for x in parsed if str(x).strip()]
    return []


PROGRAM_LABEL_RE = re.compile(r"^\s*([A-Za-z]+)(\d+)\s+(.*)$")
MINOR_LABEL_RE = re.compile(r"^\s*Minor\s+(Autumn|Spring)\s+Semester\s+(.*)$", re.IGNORECASE)
PROJECT_LABEL_RE = re.compile(r"^\s*MA\s+Project\s+(Autumn|Spring)\s+(.*)$", re.IGNORECASE)
EDOC_LABEL_RE = re.compile(r"^\s*(edoc|phd)\s+(.*)$", re.IGNORECASE)


def normalize_degree(token: str) -> str | None:
    token_up = token.upper()
    if token_up in {"BA"}:
        return "BA"
    if token_up in {"MA"}:
        return "MA"
    if token_up in {"PHD", "EDOC", "PHD."}:
        return "PhD"
    return None


def parse_program_label(label: str) -> Tuple[str, int | None, str, str] | None:
    if not label:
        return None
    match = PROGRAM_LABEL_RE.match(label)
    if not match:
        minor_match = MINOR_LABEL_RE.match(label)
        if minor_match:
            season, program_name = minor_match.groups()
            program = program_name.strip()
            if not program:
                return None
            level_label = f"Minor {season.capitalize()} Semester"
            return 'MA', None, level_label, program

        project_match = PROJECT_LABEL_RE.match(label)
        if project_match:
            season, program_name = project_match.groups()
            program = program_name.strip()
            if not program:
                return None
            level_label = f"MA Project {season.capitalize()}"
            return 'MA', None, level_label, program

        edoc_match = EDOC_LABEL_RE.match(label)
        if edoc_match:
            program = edoc_match.group(2).strip()
            if not program:
                return None
            return 'PhD', None, 'edoc', program
        return None

    degree_raw, semester_raw, program_name = match.groups()
    degree = normalize_degree(degree_raw)
    if degree is None:
        return None
    try:
        semester = int(semester_raw)
    except ValueError:
        return None
    program = program_name.strip()
    if not program:
        return None
    level_label = f"{degree}{semester}"
    return degree, semester, level_label, program


def coalesce(*values: Any, fallback: Any) -> Any:
    for v in values:
        if isinstance(v, str):
            if v.strip():
                return v.strip()
        elif v is not None:
            return v
    return fallback


def load_scores(path: Path) -> Dict[str, Dict[str, Any]]:
    scores: Dict[str, Dict[str, Any]] = {}
    with path.open(newline="", encoding="utf-8") as handle:
        reader = csv.DictReader(handle)
        for row in reader:
            row_id = row.get("row_id")
            if not row_id:
                continue
            embeddings_raw = row.get("embeddings") or row.get("embedding")
            embeddings_vec = None
            if embeddings_raw:
                try:
                    parsed = json.loads(embeddings_raw)
                    if isinstance(parsed, list):
                        try:
                            embeddings_vec = [float(x) for x in parsed]
                        except (TypeError, ValueError):
                            embeddings_vec = None
                except json.JSONDecodeError:
                    embeddings_vec = None
            scores[row_id] = {
                "score_skills_sigmoid": parse_float(row.get("score_skills_sigmoid")),
                "score_product_sigmoid": parse_float(row.get("score_product_sigmoid")),
                "score_venture_sigmoid": parse_float(row.get("score_venture_sigmoid")),
                "score_foundations_sigmoid": parse_float(row.get("score_foundations_sigmoid")),
                "embeddings": embeddings_vec,
            }
    return scores


def parse_float(value: str | None) -> float | None:
    if value is None or value == "":
        return None
    try:
        return float(value)
    except ValueError:
        return None


def build_payloads(
    courses_csv: Path, scores_csv: Path
) -> Tuple[
    Dict[str, Dict[str, Any]],  # courses_map by course_code
    List[Dict[str, Any]],       # offerings rows (with course_code, row_id, ...)
    Dict[str, set[str]],        # keywords per course_code (for tag creation)
    set[Tuple[str, str]],       # offering_keyword_links: (row_id, keyword)
    set[Tuple[str, str, str, int | None, str]],  # offering_program_links: (row_id, degree, level_label, semester, program_name)
    Dict[str, Dict[str, Any]],  # embeddings per course_code
    set[str],                   # unparsed program labels (for logging)
]:
    if not courses_csv.exists():
        raise FileNotFoundError(f"Missing CSV {courses_csv}")
    if not scores_csv.exists():
        raise FileNotFoundError(f"Missing CSV {scores_csv}")

    scores_map = load_scores(scores_csv)

    courses_map: Dict[str, Dict[str, Any]] = {}
    offerings: List[Dict[str, Any]] = []
    keywords_map: Dict[str, set[str]] = defaultdict(set)
    keyword_tag_links: set[Tuple[str, str]] = set()
    offering_program_links: set[Tuple[str, str, str, int | None, str]] = set()
    embeddings_map: Dict[str, Dict[str, Any]] = {}
    unparsed_program_labels: set[str] = set()

    with courses_csv.open(newline="", encoding="utf-8") as handle:
        reader = csv.DictReader(handle)
        for row in reader:
            course_code = row.get("course_code", "").strip()
            if not course_code:
                continue
            course = courses_map.setdefault(course_code, {
                "course_code": course_code,
                "course_name": row.get("course_name", "").strip() or course_code,
                "course_url": row.get("course_url", "").strip() or None,
                "credits": int(parse_float(row.get("credits")) or 0),
                "lang": coalesce(row.get("lang"), fallback="unknown"),
                "semester": coalesce(row.get("semester"), fallback="unknown"),
                "exam_form": row.get("exam_form", "").strip() or None,
                "workload": row.get("workload", "").strip() or None,
            })

            # Fill missing non-nullable fields if we find better values later
            if course.get("course_name") == course_code and row.get("course_name"):
                course["course_name"] = row["course_name"].strip()
            if course.get("course_url") is None and row.get("course_url"):
                course["course_url"] = row.get("course_url").strip() or None
            if course.get("lang") == "unknown" and row.get("lang"):
                course["lang"] = row["lang"].strip()
            if course.get("semester") == "unknown" and row.get("semester"):
                course["semester"] = row["semester"].strip()
            if not course.get("exam_form") and row.get("exam_form"):
                course["exam_form"] = row["exam_form"].strip()
            if not course.get("workload") and row.get("workload"):
                course["workload"] = row["workload"].strip()

            row_id = row.get("row_id", "").strip()
            if row_id:
                score = scores_map.get(row_id, {})
                offerings.append({
                    "course_code": course_code,
                    "row_id": row_id,
                    "section": row.get("section", "").strip() or "",
                    "type": row.get("type", "mandatory").strip() or "mandatory",
                    "prof_name": row.get("prof_name", "").strip() or None,
                    "score_skills_sigmoid": score.get("score_skills_sigmoid"),
                    "score_product_sigmoid": score.get("score_product_sigmoid"),
                    "score_venture_sigmoid": score.get("score_venture_sigmoid"),
                    "score_foundations_sigmoid": score.get("score_foundations_sigmoid"),
                    "embeddings": score.get("embeddings"),
                })
                emb = score.get("embeddings")
                if emb and course_code not in embeddings_map:
                    preview = (row.get("text") or "").strip()
                    embeddings_map[course_code] = {
                        "embedding": emb,
                        "preview": preview[:1000] if preview else None,
                        "row_id": row_id,
                    }

            keywords = parse_list_field(row.get("keywords", ""))
            for kw in keywords:
                keywords_map[course_code].add(kw)
                if row_id:
                    keyword_tag_links.add((row_id, kw))

            programs = parse_list_field(row.get("available_programs", ""))
            for prog in programs:
                parsed = parse_program_label(prog)
                if parsed and row_id:
                    degree, semester, level_label, program_name = parsed
                    offering_program_links.add((row_id, degree, level_label, semester, program_name))
                else:
                    unparsed_program_labels.add(prog)

    return (
        courses_map,
        offerings,
        keywords_map,
        keyword_tag_links,
        offering_program_links,
        embeddings_map,
        unparsed_program_labels,
    )


def main() -> None:
    args = parse_args()
    if not args.supabase_url or not args.service_role_key:
        print("Supabase URL and service role key are required", file=sys.stderr)
        sys.exit(1)

    client = SupabaseClient(args.supabase_url, args.service_role_key, args.batch_size)

    (
        courses_map,
        offerings,
        keywords_map,
        keyword_tag_links,
        offering_program_links,
        embeddings_map,
        unparsed_program_labels,
    ) = build_payloads(COURSES_CSV, SCORES_CSV)

    print(f"Preparing to upsert {len(courses_map)} courses, {len(offerings)} offerings")

    # 1. Upsert courses and capture IDs
    course_rows = list(courses_map.values())
    upserted_courses = client.upsert("courses", course_rows, on_conflict="course_code")
    course_ids: Dict[str, int] = {}
    for row in upserted_courses:
        course_ids[row["course_code"]] = row["id"]
    if len(course_ids) != len(courses_map):
        print("Warning: upserted course count mismatch; fetching remaining ids")
        rows = client.select("courses", columns="id,course_code")
        for row in rows:
            if row["course_code"] in courses_map:
                course_ids[row["course_code"]] = row["id"]

    # 2. Ensure tags exist
    tag_types = {item["name"]: item["id"] for item in client.select("tag_types", columns="id,name")}
    required_tag_types = {"keywords"}
    missing_types = required_tag_types - tag_types.keys()
    if missing_types:
        raise RuntimeError(f"Missing tag types in Supabase: {missing_types}")

    tag_rows = []
    tag_pairs: set[Tuple[int, str]] = set()
    for _course_code, tags in keywords_map.items():
        for name in tags:
            key = (tag_types["keywords"], name)
            if key in tag_pairs:
                continue
            tag_pairs.add(key)
            tag_rows.append({"tag_type_id": key[0], "name": name})

    if tag_rows:
        client.upsert("tags", tag_rows, on_conflict="tag_type_id,name")

    # Fetch tag IDs for mapping (page through all rows to avoid 1000-row default cap)
    all_tags = client.select_all("tags", columns="id,name,tag_type_id")
    tag_lookup: Dict[Tuple[int, str], int] = {}
    for tag in all_tags:
        tag_lookup[(tag["tag_type_id"], tag["name"])] = tag["id"]

    # 3. Upsert course_offerings with resolved course_id
    offering_rows = []
    for item in offerings:
        course_id = course_ids.get(item["course_code"])
        if not course_id:
            continue
        offering_rows.append({
            "course_id": course_id,
            "row_id": item["row_id"],
            "section": item["section"],
            "type": item["type"],
            "prof_name": item["prof_name"],
            "score_skills_sigmoid": item["score_skills_sigmoid"],
            "score_product_sigmoid": item["score_product_sigmoid"],
            "score_venture_sigmoid": item["score_venture_sigmoid"],
            "score_foundations_sigmoid": item["score_foundations_sigmoid"],
        })

    if offering_rows:
        client.upsert("course_offerings", offering_rows, on_conflict="row_id")

    # 4. Link offerings to tags (offering_tags)
    #    First, resolve offering row_id -> offering id
    offering_rows_all = client.select_all("course_offerings", columns="id,row_id")
    rowid_to_offid: Dict[str, int] = {r["row_id"]: r["id"] for r in offering_rows_all}

    offering_tag_rows = []
    seen_pairs: set[Tuple[int, int]] = set()
    for row_id, keyword in keyword_tag_links:
        off_id = rowid_to_offid.get(row_id)
        tag_id = tag_lookup.get((tag_types["keywords"], keyword))
        if not off_id or not tag_id:
            continue
        key = (off_id, tag_id)
        if key in seen_pairs:
            continue
        seen_pairs.add(key)
        offering_tag_rows.append({"offering_id": off_id, "tag_id": tag_id})

    if offering_tag_rows:
        client.insert_ignore("offering_tags", offering_tag_rows)

    embedding_rows = []
    for course_code, data in embeddings_map.items():
        course_id = course_ids.get(course_code)
        embedding_vec = data.get("embedding")
        if not course_id or embedding_vec is None:
            continue
        preview_text = data.get("preview")
        source_url = courses_map.get(course_code, {}).get("course_url")
        embedding_rows.append({
            "course_id": course_id,
            "embedding": embedding_vec,
            "preview": preview_text,
            "source_url": source_url,
        })
    if embedding_rows:
        client.upsert("course_embeddings", embedding_rows, on_conflict="course_id")

    # Summary
    # 5. Structured program / level data
    program_rows = [
        {"name": program_name.strip()}
        for program_name in sorted({entry[4].strip() for entry in offering_program_links if entry[4].strip()})
    ]
    if program_rows:
        client.upsert("programs", program_rows, on_conflict="name")

    level_rows = [
        {"degree": degree, "semester": semester, "label": level_label}
        for degree, semester, level_label in sorted({(entry[1], entry[3] if entry[3] is not None else 10_000, entry[2]) for entry in offering_program_links})
    ]
    if level_rows:
        client.upsert("levels", level_rows, on_conflict="degree,label")

    program_lookup = {row["name"]: row["id"] for row in client.select_all("programs", columns="id,name")}
    level_lookup = {
        (row["degree"], row["label"]): row["id"]
        for row in client.select_all("levels", columns="id,degree,label")
    }

    opl_rows = []
    seen_opl: set[Tuple[int, int, int]] = set()
    for row_id, degree, level_label, semester, program_name in offering_program_links:
        off_id = rowid_to_offid.get(row_id)
        program_id = program_lookup.get(program_name)
        level_id = level_lookup.get((degree, level_label))
        if not off_id or program_id is None or level_id is None:
            continue
        key = (off_id, program_id, level_id)
        if key in seen_opl:
            continue
        seen_opl.add(key)
        opl_rows.append({
            "offering_id": off_id,
            "program_id": program_id,
            "level_id": level_id,
        })

    if opl_rows:
        client.insert_ignore("offering_program_levels", opl_rows)

    kw_count = len(keyword_tag_links)
    structured_count = len(offering_program_links)
    embedding_count = len(embedding_rows)
    print(
        f"Linked offering↔tags (keywords): {kw_count} (rows inserted may be fewer due to dedupe)"
    )
    print(
        f"Linked offering↔program levels: {structured_count} (rows inserted may be fewer due to dedupe)"
    )
    print(
        f"Upserted course embeddings: {embedding_count}"
    )

    if unparsed_program_labels:
        print(
            f"Warning: skipped {len(unparsed_program_labels)} available_program labels that could not be parsed."
        )
        for label in sorted(unparsed_program_labels):
            print(f"  - {label}")

    print("Import completed")


if __name__ == "__main__":
    try:
        main()
    except Exception as exc:  # pylint: disable=broad-except
        print(f"Error: {exc}", file=sys.stderr)
        sys.exit(1)
